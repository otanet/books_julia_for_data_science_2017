{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Julia_for_Data_Science_2017.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxrwCIbBf2/1wq64Dkeca+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otanet/books_julia_for_data_science_2017/blob/main/Sample_Code_of_Julia_for_Data_Science_2017.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUVS3hlwPgqx"
      },
      "source": [
        "## Julia for Data Science 「Juliaデータサイエンス―Juliaを使って自分でゼロから作るデータサイエンス世界の探索」– 2017/9/26\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8uvKBDxPySX"
      },
      "source": [
        "### 1章"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj4g6GeEPh7O"
      },
      "source": [
        "using DataFrames, Gadfly\n",
        "p = plot(x=randn(2000), Geom.histogram(bincount=100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OwzUjkSPieq"
      },
      "source": [
        "nheads = @parallel (+) for i=1:100000000\n",
        "  Int(rand(Bool))\n",
        "end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDQDtUUPP4P1"
      },
      "source": [
        "### 2章"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oICqbV9aPimb"
      },
      "source": [
        "Pkg.update()\n",
        "Pkg.add(\"DataFrames\")\n",
        "x = DataArray([1.1, 2.2, 3.3, 4.4, 5.5, 6.6])\n",
        "x[1] = NA\n",
        "true || x\n",
        "mean(x)\n",
        "mean(x[2:6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raAECXAhPirI"
      },
      "source": [
        "Pkg.add(\"RDatasets\")\n",
        "using DataFrames\n",
        "df = DataFrame(Name = [\"Ajava Rhodiumhi\", \"Las Hushjoin\"], Count = [14.04, 17.3], OS = [\"Ubuntu\", \"Mint\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hia53XYPiuu"
      },
      "source": [
        "using RDatasets\n",
        "iris_dataset = dataset(\"datasets\", \"iris\")\n",
        "head(iris_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We5uPGeYPiyW"
      },
      "source": [
        "using DataFrames\n",
        "df2 = DataFrame()\n",
        "df2[:X] = 1:10\n",
        "df2[:Y] = [\"Head\", \"Tail\", \"Head\", \"Head\", \"Tail\", \"Head\", \"Tail\", \"Tail\", \"Head\", \"Tail\"]\n",
        "df2\n",
        "size(df2)\n",
        "head(df2)\n",
        "tail(df2)\n",
        "describe(df2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mWc55puPi2g"
      },
      "source": [
        "using DataFrames\n",
        "dvector = data([1,2,3,4,5])\n",
        "dmatrix = data([10 20 30; 40 50 60])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IynsXByRPi67"
      },
      "source": [
        "using DataFrames\n",
        "# Dataset should be there as mentioned in the chapter\n",
        "DfTRoadSafety_Accidents_2014 = readtable(\"DfTRoadSafety_Accidents_2014.csv\")\n",
        "DfTRoadSafety_Vehicles_2014 = readtable(\"DfTRoadSafety_Vehicles_2014.csv\")\n",
        "left_DfTRoadSafety_2014 = join(DfTRoadSafety_Accidents_2014, DfTRoadSafety_Vehicles_2014, on = :_Accident_Index, kind = :left)\n",
        "by(DfTRoadSafety_Accidents_2014, :Location_Northing_OSGR, size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X-jsHSZPi-w"
      },
      "source": [
        "using RDatasets, DataFrames\n",
        "iris_dataframe = dataset(\"datasets\", \"iris\")\n",
        "\n",
        "iris_dataframe[:id] = 1:size(iris_dataframe, 1)\n",
        "iris_stack = stack(iris, [1:4])\n",
        "\n",
        "iris_stack_petal = stack(iris_dataframe, [:PetalLength, :PetalWidth], :Species)\n",
        "\n",
        "iris_melt = melt(iris_dataframe, :Species)\n",
        "\n",
        "iris_default_stack = stack(iris_dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyaw-ls2PjCL"
      },
      "source": [
        "using RDatasets\n",
        "using DataFrames\n",
        "random_dataframe = DataFrame(A = randn(5), B = randn(5), C = randn(5))\n",
        "random_modelframe = ModelFrame(A ~ B + C, random_dataframe)\n",
        "random_modelmatrix = ModelMatrix(ModelFrame(A ~ B + C, random_dataframe))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfknQy3OPjFj"
      },
      "source": [
        "using DataFrames\n",
        "datavector = @data([\"A\", \"A\", \"A\",\"B\", \"B\", \"B\"])\n",
        "pooleddatavector = @pdata([\"A\", \"A\", \"A\",\"B\", \"B\", \"B\"])\n",
        "\n",
        "levels(pooleddatavector)\n",
        "\n",
        "pooleddatavector = compact(pooleddatavector)\n",
        "\n",
        "dataframe_notpooled = DataFrame(A = [10, 10, 10, 20, 20, 20], B = [\"X\", \"X\", \"X\", \"Y\", \"Y\", \"Y\"])\n",
        "\n",
        "pooleddf = pool!(dataframe_notpooled, [:A, :B])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcOBs1iJPjKU"
      },
      "source": [
        "using JSON, Requests\n",
        "reddit_url = \"https://www.reddit.com/r/Julia/\"\n",
        "response = get(\"$(reddit_url)/.json\")\n",
        "dataReceived = JSON.parse(Requests.text(response))\n",
        "nextRecord = dataReceived[\"data\"][\"after\"]\n",
        "counter = length(dataReceived[\"data\"][\"children\"])\n",
        "\n",
        "allPosts = []\n",
        "for record in 1:counter\n",
        "    url = dataReceived[\"data\"][\"children\"][record][\"data\"][\"url\"]\n",
        "    redditrecord_id  = dataReceived[\"data\"][\"children\"][record][\"data\"][\"id\"]\n",
        "    redditrecord_title  = dataReceived[\"data\"][\"children\"][record][\"data\"][\"title\"]\n",
        "    author  = dataReceived[\"data\"][\"children\"][record][\"data\"][\"author\"]\n",
        "    created = dataReceived[\"data\"][\"children\"][record][\"data\"][\"created\"]\n",
        "    push!(allPosts, (url, redditrecord_id, redditrecord_title, author, created))\n",
        "end\n",
        "\n",
        "for post in allPosts\n",
        "    println(post[3])\n",
        "end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvkcO3oUQho4"
      },
      "source": [
        "### 3章"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDyeiUfjPjNs"
      },
      "source": [
        "Pkg.update()\n",
        "Pkg.add(\"StatsBase\")\n",
        "using StatsBase\n",
        "using RDatasets\n",
        "iris_dataframe = dataset(\"datasets\", \"iris\")\n",
        "sample(iris_dataframe[:SepalLength], 5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxuxFJGrQm0O"
      },
      "source": [
        "using StatsBase\n",
        "using RDatasets\n",
        "w = WeightVec([1.,2.,3.],6.)\n",
        "eltype(wv)\n",
        "length(wv)\n",
        "isempty(wv)\n",
        "values(wv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAnnw5yXQm8t"
      },
      "source": [
        "using StatsBase\n",
        "using RDatasets\n",
        "iris_dataframe = dataset(\"datasets\", \"iris\")\n",
        "head(iris_dataframe)\n",
        "typeof(iris_dataframe[1,:Species])\n",
        "describe(iris_dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvFIKPhqQnE4"
      },
      "source": [
        "using StatsBase\n",
        "using RDatasets\n",
        "a = [123,4234,23423,1231231,1432432423,1341413413]\n",
        "geomean(a)\n",
        "harmmean(a)\n",
        "trimmean(a,0.1)\n",
        "\n",
        "wv = rand(6)\n",
        "mean(a, weights(wv))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBcIrMs7QnMR"
      },
      "source": [
        "using StatsBase\n",
        "using RDatasets\n",
        "a = [123,4234,23423,1231231,1432432423,1341413413]\n",
        "var(a)\n",
        "var(a, 2)\n",
        "std(a)\n",
        "mean_and_var(a)\n",
        "mean_and_std(a)\n",
        "skewness(a)\n",
        "kurtosis(a)\n",
        "moment(a,3)\n",
        "span(a)\n",
        "variation(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkcpWLNsQnUR"
      },
      "source": [
        "using StatsBase\n",
        "using RDatasets\n",
        "using Distributions\n",
        "d = Dirichlet([1.0, 3.0, 5.0])\n",
        "arr=rand(d)\n",
        "entropy(arr,2)\n",
        "entropy(arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlEMlwBGQnbc"
      },
      "source": [
        "using StatsBase\n",
        "using RDatasets\n",
        "using Distributions\n",
        "a = rand(4)\n",
        "ordinalrank(a)\n",
        "a = rand([1:5],30)\n",
        "counts(a)\n",
        "proportions(a,1:3)\n",
        "countmap(a)\n",
        "proportionmap(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bExSwIBYQnhu"
      },
      "source": [
        "using StatsBase\n",
        "using RDatasets\n",
        "using Distributions\n",
        "using Gadfly\n",
        "sleep = dataset(\"lme4\",\"sleepstudy\")\n",
        "plot(x = sleep[:Reaction], Geom.histogram(bincount = 30), color = sleep[:Days])\n",
        "plot(sleep, x = \"Days\", y = \"Reaction\", Geom.point)\n",
        "plot(sleep, x = \"Reaction\", Geom.density, color = \"Subject\")\n",
        "\n",
        "plot([sin, cos], 0, 25)\n",
        "\n",
        "plot(dataset(\"datasets\", \"iris\"),x=\"SepalLength\", y=\"SepalWidth\", Geom.point)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBSWpDirQ9Uh"
      },
      "source": [
        "### 4章"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C4sCZxJQnoa"
      },
      "source": [
        "using DataFrames\n",
        "using RDatasets\n",
        "using Distributions\n",
        "using Gadfly\n",
        "\n",
        "srand(619)\n",
        "\n",
        "names(Normal)\n",
        "\n",
        "params(dist1)\n",
        "\n",
        "dist1.μ\n",
        "\n",
        "dist1.σ\n",
        "\n",
        "x = rand(dist1, 1000)\n",
        "\n",
        "dist2 = Truncated(dist1, -4.0, 6.0)\n",
        "\n",
        "plot(x = rand(dist1, 1000), Geom.histogram(bincount = 30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGvN18HnRpUP"
      },
      "source": [
        "### 5章"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zeGETrkQnsu"
      },
      "source": [
        "\n",
        "using Gadfly\n",
        "\n",
        "p = plot(x=randn(2000), Geom.histogram(bincount=100))\n",
        "\n",
        "plot(x=rand(100), y=rand(100))\n",
        "\n",
        "plot(x=rand(20), y=rand(20), Geom.point, Geom.line)\n",
        "\n",
        "plot(x=1:20, y=3.^rand(20),\n",
        "     Scale.y_sqrt, Geom.point, Geom.smooth,\n",
        "     Guide.xlabel(\"This is the X label\"),\n",
        "     Guide.ylabel(\"This is the y label\"),\n",
        "     Guide.title(\"This is the title\"))\n",
        "\n",
        "?SVGJS\n",
        "\n",
        "?Gadfly.plot\n",
        "\n",
        "using RDatasets\n",
        "\n",
        "plot(dataset(\"datasets\", \"iris\"),\n",
        "    x=\"SepalLength\", y=\"SepalWidth\", Geom.point)\n",
        "\n",
        "plot(dataset(\"car\", \"SLID\"),\n",
        "    x=\"Wages\", color=\"Language\", Geom.histogram)\n",
        "\n",
        "plot(dataset(\"mlmRev\",\"Gcsemv\"),\n",
        "    x=\"Course\", color=\"Gender\", Geom.histogram)\n",
        "\n",
        "dd  =  plot(x =  rand(10),  y = rand(10));\n",
        "draw(SVG(\"random-pts.svg\",  15cm, 12cm) , dd);\n",
        "\n",
        "set_default_plot_size(15cm, 9cm);\n",
        "mlmf = dataset(\"mlmRev\",\"Gcsemv\")\n",
        "df = mlmf[complete_cases(mlmf), :]\n",
        "names(df)\n",
        "plot(df, x=\"Course\", y=\"Written\", color=\"Gender\")\n",
        "\n",
        "plot((x,y) -> x*exp(-(x-int(x))^3-y^2), -10., 10, -3., 3)\n",
        "\n",
        "plot([sin, cos], 0, 30)\n",
        "\n",
        "x = [1:100;];\n",
        "y1 = 1 - 2*rand(100);\n",
        "y2 = randn(100);\n",
        "plot(\n",
        " layer(x=x,y=y1,Geom.line,Theme(default_color=color(\"red\"))),\n",
        " layer(x=x,y=y2,Geom.line,Theme(default_color=color(\"blue\")))\n",
        ")\n",
        "\n",
        "p1 = plot(x=rand(1,1,10), y=rand(1,1,10))\n",
        "p2 = plot(x=rand(10,1,20), y=rand(10,1,20))\n",
        "draw(PNG(\"p1and2.png\", 6inch, 6inch), vstack(p1,p2))\n",
        "p3 = plot(x=rand(20,1,30), y=rand(20,1,30))\n",
        "p4 = plot(x=rand(30,1,40), y=rand(30,1,40))\n",
        "draw(PNG(\"p1to4.png\", 6inch, 9inch), vstack(hstack(p1,p2),hstack(p3,p4)))\n",
        "\n",
        "shapes = Gadfly.compose(Gadfly.context(), fill(\"cornflowerblue\"),\n",
        "          (Gadfly.context( 0.1, 0.1, 0.15, 0.1 ),  Gadfly.circle()),\n",
        "          (Gadfly.context( 0.35, 0.06, 0.2, 0.18 ),\n",
        "          Gadfly.rectangle(), Gadfly.fill(\"red\")),\n",
        "          (Gadfly.context( 0.6, 0.05, 0.2, 0.2), Gadfly.fill(\"magenta3\"),\n",
        "          Gadfly.polygon([(1, 1), (0.3, 1), (0.5, 0)]) ));\n",
        "img = SVG(\"shapes.svg\", 10cm, 8.66cm)\n",
        "draw(img,shapes)\n",
        "\n",
        "using Compose\n",
        "function sierpinski(n)\n",
        "  if n == 0\n",
        "    compose(context(), polygon([(1,1), (0,1), (1/2, 0)]));\n",
        "  else \n",
        "    t = sierpinski(n - 1);\n",
        "    compose( context(), (context( 1/4, 0, 1/2, 1/2), t),\n",
        "                        (context( 0, 1/2, 1/2, 1/2), t),\n",
        "                        (context( 1/2, 1/2, 1/2, 1/2), t));\n",
        "  end\n",
        "end\n",
        "\n",
        "cx1 = compose(sierpinski(1), linewidth(0.2mm),\n",
        "  fill(nothing), stroke(\"black\"));\n",
        "img = SVG(\"sierp1.svg\", 10cm, 8.66cm); draw(img,cx1)\n",
        "\n",
        "cx3 = compose(sierpinski(3), linewidth(0.2mm),\n",
        "  fill(nothing), stroke(\"black\"));\n",
        "img = SVG(\"sierp3.svg\", 10cm, 8.66cm); draw(img,cx3)\n",
        "\n",
        "cx5 = compose(sierpinski(5), linewidth(0.2mm),\n",
        "  fill(nothing), stroke(\"black\"));\n",
        "img = SVG(\"sierp5.svg\", 10cm, 8.66cm); draw(img,cx5)\n",
        "\n",
        "plot(x=rand(30), y=rand(30), Stat.step, Geom.line)\n",
        "\n",
        "using Distributions\n",
        "plot(x=rand(Normal(), 150), y=rand(Normal(), 150), Stat.qq, Geom.point)\n",
        "plot(x=rand(Normal(), 150), y=Normal(), Stat.qq, Geom.point)\n",
        "\n",
        "# Providing a fixed set of ticks\n",
        "plot(x=rand(20), y=rand(20),\n",
        "     Stat.xticks(ticks=[0.0, 0.2, 0.8, 1.0]),\n",
        "     Stat.yticks(ticks=[0.0, 0.1, 0.9, 1.0]),\n",
        "     Geom.point)\n",
        "\n",
        "plot(dataset(\"lattice\", \"singer\"),\n",
        "    x=\"VoicePart\", y=\"Height\", Geom.boxplot)\n",
        "\n",
        "plot(dataset(\"ggplot2\", \"diamonds\"),\n",
        "    x=\"Price\", color=\"Cut\", Geom.density)\n",
        "\n",
        "plot(dataset(\"HistData\", \"ChestSizes\"),\n",
        "    x=\"Chest\", y=\"Count\", Geom.bar)\n",
        "\n",
        "# Explicitly setting the number of bins\n",
        "plot(dataset(\"car\", \"UN\"), x=\"GDP\", y=\"InfantMortality\",\n",
        "     Scale.x_log10, Scale.y_log10,\n",
        "     Geom.histogram2d(xbincount=20, ybincount=20))\n",
        "\n",
        "x_data = 0.0:0.1:3.0\n",
        "y_data = x_data.^2 + rand(length(x_data))\n",
        "plot(x=x_data, y=y_data,\n",
        "    Geom.point,\n",
        "    Geom.smooth(method=:loess,smoothing=0.9))\n",
        "\n",
        "set_default_plot_size(20cm, 7.5cm)\n",
        "plot(dataset(\"datasets\", \"OrchardSprays\"),\n",
        "     xgroup=\"Treatment\", x=\"ColPos\", y=\"RowPos\", color=\"Decrease\",\n",
        "     Geom.subplot_grid(Geom.point))\n",
        "\n",
        "using DataFrames\n",
        "set_default_plot_size(8cm, 12cm)\n",
        "widedf = DataFrame(x = [1:10], var1 = [1:10], var2 = [1:10].^2)\n",
        "longdf = stack(widedf, [:var1, :var2])\n",
        "plot(longdf, ygroup=\"variable\", x=\"x\", y=\"value\",\n",
        "    Geom.subplot_grid(Geom.point, free_y_axis=true))\n",
        "\n",
        "set_default_plot_size(15cm, 10cm)\n",
        "\n",
        "plot(dataset(\"datasets\", \"iris\"), x=\"SepalLength\", y=\"SepalWidth\",\n",
        "     yintercept=[2.5, 4.0], Geom.point, Geom.hline,\n",
        "     xintercept=[5.0, 7.0], Geom.point, Geom.vline)\n",
        "\n",
        "xs = 0:0.1:20\n",
        "\n",
        "df_cos = DataFrame(\n",
        "    x=xs,\n",
        "    y=cos(xs),\n",
        "    ymin=cos(xs) .- 0.5,\n",
        "    ymax=cos(xs) .+ 0.5,\n",
        "    f=\"cos\"\n",
        ")\n",
        "\n",
        "df_sin = DataFrame(\n",
        "    x=xs,\n",
        "    y=sin(xs),\n",
        "    ymin=sin(xs) .- 0.5,\n",
        "    ymax=sin(xs) .+ 0.5,\n",
        "    f=\"sin\"\n",
        ")\n",
        "\n",
        "df = vcat(df_cos, df_sin)\n",
        "p = plot(df, x=:x, y=:y, ymin=:ymin, ymax=:ymax, color=:f, Geom.line, Geom.ribbon)\n",
        "\n",
        "plot(dataset(\"lattice\", \"singer\"),\n",
        "    x=\"VoicePart\", y=\"Height\", Geom.violin)\n",
        "\n",
        "# Binding categorial data to x\n",
        "plot(dataset(\"lattice\", \"singer\"),\n",
        "    x=\"VoicePart\", y=\"Height\", Geom.beeswarm)\n",
        "\n",
        "# Transform both dimensions\n",
        "plot(x=rand(100), y=rand(100),\n",
        "    Scale.x_log, Scale.y_log)\n",
        "\n",
        "# Treat numerical y data as categories\n",
        "plot(x=rand(20), y=rand(20),\n",
        "    Scale.x_discrete)\n",
        "\n",
        "using Colors\n",
        "x = repeat([1:10], inner=[10])\n",
        "y = repeat([1:10], outer=[10])\n",
        "plot(x=x,y=y,color=x+y, Geom.rectbin,\n",
        "     Scale.ContinuousColorScale(Scale.lab_gradient(colorant\"green\",\n",
        "                                                   colorant\"white\",\n",
        "                                                   colorant\"red\")))\n",
        "\n",
        "plot(sin, 0, 2pi,\n",
        "     Guide.annotation(\n",
        "       compose(context(), circle([pi/2, 3*pi/2], [1.0, -1.0], [2mm]), fill(nothing),\n",
        "       stroke(colorant\"orange\"))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZECc2DtQnwq"
      },
      "source": [
        "\n",
        "using PyPlot\n",
        "PyPlot.svg(true)\n",
        "\n",
        "x = [1:100]\n",
        "y = [i^2 for i in x]\n",
        "p = plot(x,y)\n",
        "xlabel(\"X\")\n",
        "ylabel(\"Y\")\n",
        "title(\"Basic plot\")\n",
        "grid(\"on\")\n",
        "\n",
        "x = linspace(0, 3pi, 1000)\n",
        "y = cos(2*x + 3*sin(3*x));\n",
        "plot(x, y, color=\"orange\", linewidth=2.0, linestyle=\"--\");\n",
        "title(\"Another plot using sine and cosine\");\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X = linspace(0, 2 * pi, 100)\n",
        "Ya = sin(X)\n",
        "Yb = cos(X)\n",
        "\n",
        "plot(X, Ya)\n",
        "plot(X, Yb)\n",
        "show()\n",
        "\n",
        "Pkg.add(\"UnicodePlots\")\n",
        "\n",
        "using UnicodePlots\n",
        "\n",
        "myPlot = lineplot([1, 2, 3, 7], [1, 2, -5, 7], title=\"My Plot\", border=:dotted)\n",
        "\n",
        "myPlot = densityplot([1:100], randn(100), border=:dotted)\n",
        "\n",
        "using ASCIIPlots\n",
        "\n",
        "x = collect(-pi:0.2:pi)\n",
        "lineplot(x, sin(x))\n",
        "\n",
        "using Vega\n",
        "\n",
        "x = [0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9]\n",
        "y = [28, 43, 81, 19, 52, 24, 87, 17, 68, 49, 55, 91, 53, 87, 48, 49, 66, 27, 16, 15]\n",
        "g = [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1]\n",
        "\n",
        "a = areaplot(x = x, y = y, group = g, stacked = true)\n",
        "\n",
        "fruit = [\"peaches\", \"plums\", \"blueberries\", \"strawberries\", \"bananas\"];\n",
        "bushels = [100, 32, 180, 46, 21];\n",
        "piechart(x = fruit, y = bushels, holesize = 125)\n",
        "\n",
        "using Bokeh\n",
        "using Dates # for version 0.3\n",
        "\n",
        "start = Date(2015, 6, 21)\n",
        "days = 120\n",
        "x = map(d -> start + Dates.Day(d), 1:days)\n",
        "y = 15 + randn(days) * 4\n",
        "plot(x, y, title=\"A typical British Summer\", legends=[\"Temperature\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89bhJ4qvQn0q"
      },
      "source": [
        "\n",
        "using UnicodePlots\n",
        "\n",
        "scatterplot(randn(50), randn(50),\n",
        "    title = \"My Scatterplot\")\n",
        "\n",
        "myPlot = densityplot([1:100], randn(100), border=:dotted)\n",
        "\n",
        "lineplot(rand(3), rand(3), title = \"My Lineplot\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBL4bFJCQn4V"
      },
      "source": [
        "\n",
        "using Vega\n",
        "\n",
        "scatterplot(x = rand(100), y = rand(100))\n",
        "\n",
        "using Vega, Distributions\n",
        "\n",
        "d1 = MultivariateNormal([0.0, 0.0], [1.0 0.9; 0.9 1.0])\n",
        "d2 = MultivariateNormal([10.0, 10.0], [4.0 0.5; 0.5 4.0])\n",
        "points = vcat(rand(d1, 500)', rand(d2, 500)')\n",
        "\n",
        "x = points[:, 1]\n",
        "y = points[:, 2]\n",
        "group = vcat(ones(Int, 500), ones(Int, 500) + 1)\n",
        "\n",
        "scatterplot(x = x, y = y, group = group)\n",
        "\n",
        "x = [0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9]\n",
        "y = [28, 43, 81, 19, 52, 24, 87, 17, 68, 49, 55, 91, 53, 87, 48, 49, 66, 27, 16, 15]\n",
        "g = [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1]\n",
        "\n",
        "a = areaplot(x = x, y = y, group = g, stacked = true)\n",
        "\n",
        "fruit = [\"peaches\", \"plums\", \"blueberries\", \"strawberries\", \"bananas\"];\n",
        "bushels = [100, 32, 180, 46, 21];\n",
        "piechart(x = fruit, y = bushels, holesize = 125)\n",
        "\n",
        "x = Array(Int, 900)\n",
        "y = Array(Int, 900)\n",
        "color = Array(Float64, 900)\n",
        "tmp = 0\n",
        "for counter in 1:30\n",
        "    for counter2 in 1:30\n",
        "        tmp += 1\n",
        "        x[tmp] = counter\n",
        "        y[tmp] = counter2\n",
        "        color[tmp] = rand()\n",
        "    end\n",
        "end\n",
        "hm = heatmap(x = x, y = y, color = color)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw_IrwSJRYWC"
      },
      "source": [
        "### 6章"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CIIsztdQn8E"
      },
      "source": [
        "\n",
        "using DecisionTree\n",
        "using ScikitLearn\n",
        "using PyPlot\n",
        "\n",
        "# Create a random dataset\n",
        "srand(100)\n",
        "X = sort(5 * rand(80))\n",
        "XX = reshape(X, 80, 1)\n",
        "y = sin(X)\n",
        "y[1:5:end] += 3 * (0.5 - rand(16))\n",
        "\n",
        "# Fit regression model\n",
        "regr_1 = DecisionTreeRegressor()\n",
        "regr_2 = DecisionTreeRegressor(pruning_purity_threshold=0.05)\n",
        "\n",
        "fit!(regr_1, XX, y)\n",
        "\n",
        "fit!(regr_2, XX, y)\n",
        "\n",
        "# Predict\n",
        "X_test = 0:0.01:5.0\n",
        "y_1 = predict(regr_1, hcat(X_test))\n",
        "y_2 = predict(regr_2, hcat(X_test))\n",
        "\n",
        "# Plot the results\n",
        "scatter(X, y, c=\"k\", label=\"data\")\n",
        "plot(X_test, y_1, c=\"g\", label=\"no pruning\", linewidth=2)\n",
        "plot(X_test, y_2, c=\"r\", label=\"pruning_purity_threshold=0.05\", linewidth=2)\n",
        "\n",
        "xlabel(\"data\")\n",
        "ylabel(\"target\")\n",
        "title(\"Decision Tree Regression\")\n",
        "legend(prop=Dict(\"size\"=>10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ell4icjYPjRK"
      },
      "source": [
        "Pkg.clone(\"https://github.com/dfdx/NaiveBayes.jl.git\")\n",
        "\n",
        "using NaiveBayes\n",
        "using RDatasets\n",
        "\n",
        "iris = dataset(\"datasets\", \"iris\")\n",
        "\n",
        "# observations in columns and variables in rows\n",
        "X = array(iris[:, 1:4])'\n",
        "p, n = size(X)\n",
        "# by default species is a PooledDataArray,\n",
        "y = [species for species in iris[:, 5]]\n",
        "\n",
        "# how much data use for training\n",
        "train_frac = 0.9\n",
        "k = int(floor(train_frac * n))\n",
        "idxs = randperm(n)\n",
        "train = idxs[1:k]\n",
        "test = idxs[k+1:end]\n",
        "\n",
        "model = GaussianNB(unique(y), p)\n",
        "fit(model, X[:, train], y[train])\n",
        "\n",
        "accuracy = countnz(predict(model, X[:,test]).==\n",
        "            y[test]) / countnz(test)\n",
        "\n",
        "println(\"Accuracy: $accuracy\")\n",
        "\n",
        "X = [1 1 0 2 1;\n",
        "     0 0 3 1 0;\n",
        "     1 0 1 0 2]\n",
        "\n",
        "y = [:a, :b, :b, :a, :a]\n",
        "\n",
        "m = MultinomialNB(unique(y), 3)\n",
        "fit(m, X, y)\n",
        "\n",
        "\n",
        "Xtest = [0 4 1;\n",
        "         2 2 0;\n",
        "         1 1 1]\n",
        "\n",
        "predict(m, Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STYoubNJSFaK"
      },
      "source": [
        "### 7章"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkju4xBtPjUZ"
      },
      "source": [
        "\n",
        "Pkg.add(\"Clustering\")\n",
        "\n",
        "Pkg.update()\n",
        "\n",
        "using Clustering\n",
        "using RDatasets\n",
        "\n",
        "xclara = dataset(\"cluster\", \"xclara\")\n",
        "\n",
        "names!(xclara, [symbol(i) for i in [\"x\", \"y\"]])\n",
        "\n",
        "using Clustering\n",
        "using Gadfly\n",
        "iris = dataset(\"datasets\", \"iris\")\n",
        "features = array(iris[:, 1:4])'\n",
        "# group the data onto 3 clusters\n",
        "result = kmeans( features, 3 )\n",
        "plot(iris, x = \"PetalLength\", y = \"PetalWidth\",\n",
        "        color = result.assignments, Geom.point)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0guG5WKPjYC"
      },
      "source": [
        "\n",
        "Pkg.update()\n",
        "Pkg.add(\"ScikitLearn\")\n",
        "Pkg.add(\"PyPlot\")\n",
        "\n",
        "using ScikitLearn\n",
        "using PyPlot\n",
        "\n",
        "@sk_import datasets: (make_circles, make_moons, make_blobs)\n",
        "@sk_import cluster: (estimate_bandwidth, MeanShift, MiniBatchKMeans, AgglomerativeClustering, SpectralClustering)\n",
        "@sk_import cluster: (DBSCAN, AffinityPropagation, Birch)\n",
        "@sk_import preprocessing: StandardScaler\n",
        "@sk_import neighbors: kneighbors_graph\n",
        "\n",
        "srand(33)\n",
        "\n",
        "# Generate datasets. We choose the size big enough to see the scalability\n",
        "# of the algorithms, but not too big to avoid too long running times\n",
        "n_samples = 1500\n",
        "noisy_circles = make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
        "noisy_moons = make_moons(n_samples=n_samples, noise=.05)\n",
        "blobs = make_blobs(n_samples=n_samples, random_state=8)\n",
        "no_structure = rand(n_samples, 2), nothing\n",
        "\n",
        "colors0 = collect(\"bgrcmykbgrcmykbgrcmykbgrcmyk\")\n",
        "colors = vcat(fill(colors0, 20)...)\n",
        "\n",
        "clustering_names = [\n",
        "    \"MiniBatchKMeans\", \"AffinityPropagation\", \"MeanShift\",\n",
        "    \"SpectralClustering\", \"Ward\", \"AgglomerativeClustering\",\n",
        "    \"DBSCAN\", \"Birch\"];\n",
        "\n",
        "figure(figsize=(length(clustering_names) * 2 + 3, 9.5))\n",
        "subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
        "                    hspace=.01)\n",
        "\n",
        "plot_num = 1\n",
        "\n",
        "datasets = [noisy_circles, noisy_moons, blobs, no_structure]\n",
        "\n",
        "for (i_dataset, dataset) in enumerate(datasets)\n",
        "\n",
        "    X, y = dataset\n",
        "    # normalize dataset for easier parameter selection\n",
        "    X = fit_transform!(StandardScaler(), X)\n",
        "\n",
        "    # estimate bandwidth for mean shift\n",
        "    bandwidth = estimate_bandwidth(X, quantile=0.3)\n",
        "\n",
        "    # connectivity matrix for structured Ward\n",
        "    connectivity = kneighbors_graph(X, n_neighbors=10,\n",
        "                include_self=false)[:todense]()\n",
        "    \n",
        "    # PyCall does not support numpy sparse matrices\n",
        "    # make connectivity symmetric\n",
        "    connectivity = 0.5 * (connectivity + connectivity')\n",
        "\n",
        "    # create clustering estimators\n",
        "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=true)\n",
        "    two_means = MiniBatchKMeans(n_clusters=2)\n",
        "    ward = AgglomerativeClustering(n_clusters=2, linkage=\"ward\",\n",
        "                                   connectivity=connectivity)\n",
        "    spectral = SpectralClustering(n_clusters=2,\n",
        "                                  eigen_solver=\"arpack\",\n",
        "                                  affinity=\"nearest_neighbors\")\n",
        "    dbscan = DBSCAN(eps=.2)\n",
        "    affinity_propagation = AffinityPropagation(damping=.9, preference=-200)\n",
        "\n",
        "    average_linkage = AgglomerativeClustering(\n",
        "        linkage=\"average\", affinity=\"cityblock\", n_clusters=2,\n",
        "        connectivity=connectivity)\n",
        "\n",
        "    birch = Birch(n_clusters=2)\n",
        "    clustering_algorithms = [\n",
        "        two_means, affinity_propagation, ms, spectral, ward, average_linkage,\n",
        "        dbscan, birch]\n",
        "\n",
        "    for (name, algorithm) in zip(clustering_names, clustering_algorithms)\n",
        "        fit!(algorithm, X)\n",
        "        y_pred = nothing\n",
        "        try\n",
        "            y_pred = predict(algorithm, X)\n",
        "        catch e\n",
        "            if isa(e, KeyError)\n",
        "                y_pred = map(Int, algorithm[:labels_])\n",
        "                clamp!(y_pred, 0, 27) # not sure why some algorithms return -1\n",
        "            else rethrow() end\n",
        "        end\n",
        "        subplot(4, length(clustering_algorithms), plot_num)\n",
        "        if i_dataset == 1\n",
        "            title(name, size=18)\n",
        "        end\n",
        "\n",
        "        for y_val in unique(y_pred)\n",
        "            selected = y_pred.==y_val\n",
        "            scatter(X[selected, 1], X[selected, 2], color=string(colors0[y_val+1]), s=10)\n",
        "        end\n",
        "\n",
        "        xlim(-2, 2)\n",
        "        ylim(-2, 2)\n",
        "        xticks(())\n",
        "        yticks(())\n",
        "        plot_num += 1\n",
        "    end\n",
        "end\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q43Pos1SPrB"
      },
      "source": [
        "### 8章"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6CseITRPjbl"
      },
      "source": [
        "\n",
        "Pkg.add(\"DecisionTree\")\n",
        "\n",
        "using RDatasets: dataset\n",
        "using DecisionTree\n",
        "\n",
        "iris = dataset(\"datasets\", \"iris\")\n",
        "features = convert(Array, iris[:, 1:4]);\n",
        "labels = convert(Array, iris[:, 5]);\n",
        "\n",
        "# train full-tree classifier\n",
        "model = build_tree(labels, features)\n",
        "# prune tree: merge leaves having >= 90% combined purity (default: 100%)\n",
        "model = prune_tree(model, 0.9)\n",
        "# pretty print of the tree, to a depth of 5 nodes (optional)\n",
        "print_tree(model, 5)\n",
        "# apply learned model\n",
        "apply_tree(model, [5.9,3.0,5.1,1.9])\n",
        "# get the probability of each label\n",
        "apply_tree_proba(model, [5.9,3.0,5.1,1.9], [\"setosa\", \"versicolor\", \"virginica\"])\n",
        "# run n-fold cross validation for pruned tree,\n",
        "# using 90% purity threshold pruning, and 3 CV folds\n",
        "accuracy = nfoldCV_tree(labels, features, 0.9, 3)\n",
        "\n",
        "# train random forest classifier\n",
        "# using 2 random features, 10 trees, 0.5 portion of samples per tree (optional), and a maximum tree depth of 6 (optional)\n",
        "model = build_forest(labels, features, 2, 10, 0.5, 6)\n",
        "# apply learned model\n",
        "apply_forest(model, [5.9,3.0,5.1,1.9])\n",
        "# get the probability of each label\n",
        "apply_forest_proba(model, [5.9,3.0,5.1,1.9], [\"setosa\", \"versicolor\", \"virginica\"])\n",
        "# run n-fold cross validation for forests\n",
        "# using 2 random features, 10 trees, 3 folds and 0.5 of samples per tree (optional)\n",
        "accuracy = nfoldCV_forest(labels, features, 2, 10, 3, 0.5)\n",
        "\n",
        "# train adaptive-boosted stumps, using 7 iterations\n",
        "model, coeffs = build_adaboost_stumps(labels, features, 7);\n",
        "# apply learned model\n",
        "apply_adaboost_stumps(model, coeffs, [5.9,3.0,5.1,1.9])\n",
        "# get the probability of each label\n",
        "apply_adaboost_stumps_proba(model, coeffs, [5.9,3.0,5.1,1.9], [\"setosa\", \"versicolor\", \"virginica\"])\n",
        "# run n-fold cross validation for boosted stumps, using 7 iterations and 3 folds\n",
        "accuracy = nfoldCV_stumps(labels, features, 7, 3)\n",
        "\n",
        "n, m = 10^3, 5 ;\n",
        "features = randn(n, m);\n",
        "weights = rand(-2:2, m);\n",
        "labels = features * weights;\n",
        "\n",
        "# train regression tree, using an averaging of 5 samples per leaf (optional)\n",
        "model = build_tree(labels, features, 5)\n",
        "# apply learned model\n",
        "apply_tree(model, [-0.9,3.0,5.1,1.9,0.0])\n",
        "# run n-fold cross validation, using 3 folds, averaging of 5 samples per leaf (optional)\n",
        "# returns array of coefficients of determination (R^2)\n",
        "r2 = nfoldCV_tree(labels, features, 3, 5)\n",
        "\n",
        "# train regression forest, using 2 random features, 10 trees,\n",
        "# averaging of 5 samples per leaf (optional), 0.7 of samples per tree (optional)\n",
        "model = build_forest(labels,features, 2, 10, 5, 0.7)\n",
        "# apply learned model\n",
        "apply_forest(model, [-0.9,3.0,5.1,1.9,0.0])\n",
        "# run n-fold cross validation on regression forest\n",
        "# using 2 random features, 10 trees, 3 folds, averaging of 5 samples/leaf (optional),\n",
        "# and 0.7 porition of samples per tree (optional)\n",
        "# returns array of coefficients of determination (R^2)\n",
        "r2 = nfoldCV_forest(labels, features, 2, 10, 3, 5, 0.7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b_0dYb_SZvP"
      },
      "source": [
        "### 9章"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvVD44ndPjex"
      },
      "source": [
        "\n",
        "using TimeSeries\n",
        "\n",
        "using MarketData\n",
        "\n",
        "ohlc[1]\n",
        "\n",
        "ohlc[[1:3;9]]\n",
        "\n",
        "ohlc[\"Open\",\"Close\"]\n",
        "\n",
        "ohlc[[Date(2000,1,3),Date(2000,2,4)]]\n",
        "\n",
        "ohlc[Date(2000,1,10):Date(2000,2,10)]\n",
        "\n",
        "ohlc[\"Open\"][Date(2000,1,10)]\n",
        "\n",
        "from(cl, Date(2001, 10, 24))\n",
        "\n",
        "to(cl, Date(2000, 10, 24))\n",
        "\n",
        "red = findwhen(ohlc[\"Close\"] .< ohlc[\"Open\"]);\n",
        "\n",
        "ohlc[red]\n",
        "\n",
        "green = find(ohlc[\"Close\"] .> ohlc[\"Open\"]);\n",
        "\n",
        "ohlc[green]\n",
        "\n",
        "cl[1:4]\n",
        "\n",
        "lag(cl[1:4])\n",
        "\n",
        "lead(cl[1:4])\n",
        "\n",
        "lead(cl, 400)\n",
        "\n",
        "percentchange(cl)\n",
        "\n",
        "merge(op[1:4], cl[2:6], :left)\n",
        "\n",
        "a = TimeArray([Date(2015, 10, 24), Date(2015, 11, 04)], [15, 16], [\"Number\"])\n",
        "\n",
        "map((timestamp, values) -> (timestamp + Dates.Year(1), values), a)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GeHACCmSgvz"
      },
      "source": [
        "### 10章"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMifEeQ0Pjim"
      },
      "source": [
        "using RecSys\n",
        "\n",
        "import RecSys: train, recommend, rmse\n",
        "\n",
        "if isless(Base.VERSION, v\"0.5.0-\")\n",
        "    using SparseVectors\n",
        "end\n",
        "\n",
        "type MovieRec\n",
        "    movie_names::FileSpec\n",
        "    als::ALSWR\n",
        "    movie_mat::Nullable{SparseVector{AbstractString,Int64}}\n",
        "\n",
        "    function MovieRec(trainingset::FileSpec, movie_names::FileSpec)\n",
        "        new(movie_names, ALSWR(trainingset, ParShmem()), nothing)\n",
        "    end\n",
        "\n",
        "    function MovieRec(trainingset::FileSpec, movie_names::FileSpec, thread::Bool) \n",
        "\tnew(movie_names, ALSWR(trainingset, ParThread()), nothing)\n",
        "    end     \n",
        "\n",
        "    function MovieRec(user_item_ratings::FileSpec, item_user_ratings::FileSpec, movie_names::FileSpec)\n",
        "        new(movie_names, ALSWR(user_item_ratings, item_user_ratings, ParBlob()), nothing)\n",
        "    end\n",
        "end\n",
        "\n",
        "function movie_names(rec::MovieRec)\n",
        "    if isnull(rec.movie_mat)\n",
        "        A = read_input(rec.movie_names)\n",
        "        movie_ids = convert(Array{Int}, A[:,1])\n",
        "        movie_names = convert(Array{AbstractString}, A[:,2])\n",
        "        movie_genres = convert(Array{AbstractString}, A[:,3])\n",
        "        movies = AbstractString[n*\" - \"*g for (n,g) in zip(movie_names, movie_genres)]\n",
        "        M = SparseVector(maximum(movie_ids), movie_ids, movies)\n",
        "        rec.movie_mat = Nullable(M)\n",
        "    end\n",
        "\n",
        "    get(rec.movie_mat)\n",
        "end\n",
        "\n",
        "train(movierec::MovieRec, args...) = train(movierec.als, args...)\n",
        "rmse(movierec::MovieRec, args...; kwargs...) = rmse(movierec.als, args...; kwargs...)\n",
        "recommend(movierec::MovieRec, args...; kwargs...) = recommend(movierec.als, args...; kwargs...)\n",
        "\n",
        "function print_list(mat::SparseVector, idxs::Vector{Int}, header::AbstractString)\n",
        "    if isless(Base.VERSION, v\"0.5.0-\")\n",
        "        if !isempty(idxs)\n",
        "            println(header)\n",
        "            for idx in idxs\n",
        "                println(\"[$idx] $(mat[idx])\")\n",
        "            end\n",
        "        end\n",
        "    else\n",
        "        isempty(idxs) || println(\"$header\\n$(mat[idxs])\")\n",
        "    end\n",
        "end\n",
        "\n",
        "function print_recommendations(rec::MovieRec, recommended::Vector{Int}, watched::Vector{Int}, nexcl::Int)\n",
        "    mnames = movie_names(rec)\n",
        "\n",
        "    print_list(mnames, watched, \"Already watched:\")\n",
        "    (nexcl == 0) || println(\"Excluded $(nexcl) movies already watched\")\n",
        "    print_list(mnames, recommended, \"Recommended:\")\n",
        "    nothing\n",
        "end\n",
        "\n",
        "function test_thread(dataset_path)\n",
        "    ratings_file = DlmFile(joinpath(dataset_path, \"ratings.csv\"); dlm=',', header=true)\n",
        "    movies_file = DlmFile(joinpath(dataset_path, \"movies.csv\"); dlm=',', header=true)\n",
        "    rec = MovieRec(ratings_file, movies_file, true)\n",
        "\n",
        "    @time train(rec, 10, 10)\n",
        "end\n",
        "\n",
        "function test(dataset_path)\n",
        "    ratings_file = DlmFile(joinpath(dataset_path, \"ratings.csv\"); dlm=',', header=true)\n",
        "    movies_file = DlmFile(joinpath(dataset_path, \"movies.csv\"); dlm=',', header=true)\n",
        "    rec = MovieRec(ratings_file, movies_file)\n",
        "    @time train(rec, 10, 10)\n",
        "\n",
        "    err = rmse(rec)\n",
        "    println(\"rmse of the model: $err\")\n",
        "\n",
        "    println(\"recommending existing user:\")\n",
        "    print_recommendations(rec, recommend(rec, 100)...)\n",
        "\n",
        "    println(\"recommending anonymous user:\")\n",
        "    u_idmap = RecSys.user_idmap(rec.als.inp)\n",
        "    i_idmap = RecSys.item_idmap(rec.als.inp)\n",
        "    # take user 100\n",
        "    actual_user = isempty(u_idmap) ? 100 : findfirst(u_idmap, 100)\n",
        "    rated_anon, ratings_anon = RecSys.items_and_ratings(rec.als.inp, actual_user)\n",
        "    actual_movie_ids = isempty(i_idmap) ? rated_anon : i_idmap[rated_anon]\n",
        "    nmovies = isempty(i_idmap) ? RecSys.nitems(rec.als.inp) : maximum(i_idmap)\n",
        "    sp_ratings_anon = SparseVector(nmovies, actual_movie_ids, ratings_anon)\n",
        "    print_recommendations(rec, recommend(rec, sp_ratings_anon)...)\n",
        "\n",
        "    println(\"saving model to model.sav\")\n",
        "    clear(rec.als)\n",
        "    localize!(rec.als)\n",
        "    save(rec, \"model.sav\")\n",
        "    nothing\n",
        "end\n",
        "\n",
        "# prepare chunks for movielens dataset by running `split_movielens` from `playground/split_input.jl`\n",
        "function test_chunks(dataset_path, model_path)\n",
        "    mem = Base.Sys.free_memory()\n",
        "    mem_model = mem_inputs = round(Int, mem/3)\n",
        "    user_item_ratings = SparseBlobs(joinpath(dataset_path, \"splits\", \"R_itemwise\"); maxcache=mem_model)\n",
        "    item_user_ratings = SparseBlobs(joinpath(dataset_path, \"splits\", \"RT_userwise\"); maxcache=mem_model)\n",
        "    movies_file = DlmFile(joinpath(dataset_path, \"movies.csv\"); dlm=',', header=true)\n",
        "    rec = MovieRec(user_item_ratings, item_user_ratings, movies_file)\n",
        "    @time train(rec, 10, 10, model_path, mem_inputs)\n",
        "\n",
        "    err = rmse(rec)\n",
        "    println(\"rmse of the model: $err\")\n",
        "\n",
        "    println(\"recommending existing user:\")\n",
        "    print_recommendations(rec, recommend(rec, 100)...)\n",
        "    nothing\n",
        "end\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJHeI7t6Sms9"
      },
      "source": [
        "### 11章"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpdXbcvEPjly"
      },
      "source": [
        "using Mocha\n",
        "\n",
        "data_layer  = HDF5DataLayer(name=\"train-data\", source=\"data/train.txt\",\n",
        "    batch_size=64, shuffle=true)\n",
        "\n",
        "conv_layer = ConvolutionLayer(name=\"conv1\", n_filter=20, kernel=(5,5),\n",
        "    bottoms=[:data], tops=[:conv1])\n",
        "\n",
        "pool_layer = PoolingLayer(name=\"pool1\", kernel=(2,2), stride=(2,2),\n",
        "    bottoms=[:conv1], tops=[:pool1])\n",
        "conv2_layer = ConvolutionLayer(name=\"conv2\", n_filter=50, kernel=(5,5),\n",
        "    bottoms=[:pool1], tops=[:conv2])\n",
        "pool2_layer = PoolingLayer(name=\"pool2\", kernel=(2,2), stride=(2,2),\n",
        "    bottoms=[:conv2], tops=[:pool2])\n",
        "\n",
        "\n",
        "fc1_layer  = InnerProductLayer(name=\"ip1\", output_dim=500,\n",
        "    neuron=Neurons.ReLU(), bottoms=[:pool2], tops=[:ip1])\n",
        "fc2_layer  = InnerProductLayer(name=\"ip2\", output_dim=10,\n",
        "    bottoms=[:ip1], tops=[:ip2])\n",
        "\n",
        "\n",
        "loss_layer = SoftmaxLossLayer(name=\"loss\", bottoms=[:ip2,:label])\n",
        "\n",
        "backend = CPUBackend()\n",
        "init(backend)\n",
        "\n",
        "common_layers = [conv_layer, pool_layer, conv2_layer, pool2_layer,\n",
        "    fc1_layer, fc2_layer]\n",
        "net = Net(\"MNIST-train\", backend, [data_layer, common_layers..., loss_layer])\n",
        "\n",
        "exp_dir = \"snapshots\"\n",
        "method = SGD()\n",
        "params = make_solver_parameters(method, max_iter=10000, regu_coef=0.0005,\n",
        "    mom_policy=MomPolicy.Fixed(0.9),\n",
        "    lr_policy=LRPolicy.Inv(0.01, 0.0001, 0.75),\n",
        "    load_from=exp_dir)\n",
        "solver = Solver(method, params)\n",
        "\n",
        "\n",
        "setup_coffee_lounge(solver, save_into=\"$exp_dir/statistics.hdf5\", every_n_iter=1000)\n",
        "\n",
        "add_coffee_break(solver, TrainingSummary(), every_n_iter=100)\n",
        "add_coffee_break(solver, Snapshot(exp_dir), every_n_iter=5000)\n",
        "\n",
        "data_layer_test = HDF5DataLayer(name=\"test-data\", source=\"data/test.txt\", batch_size=100)\n",
        "acc_layer = AccuracyLayer(name=\"test-accuracy\", bottoms=[:ip2, :label])\n",
        "test_net = Net(\"MNIST-test\", backend, [data_layer_test, common_layers..., acc_layer])\n",
        "\n",
        "add_coffee_break(solver, ValidationPerformance(test_net), every_n_iter=1000)\n",
        "\n",
        "solve(solver, net)\n",
        "\n",
        "destroy(net)\n",
        "destroy(test_net)\n",
        "shutdown(backend)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbfnD5QvPjpO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjMJ7k3LPjsg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1CYl801SyLo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHm_NLiEmYwd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}